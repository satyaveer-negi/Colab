{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyaveer-negi/Colab/blob/main/Buck_Boost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNtewAmrl_2K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Step 1: Environment Setup\n",
        "\n",
        "class BuckConverterEnvironment:\n",
        "    def __init__(self, dt=1e-5, vin=24, r=10, l=1e-3, c=10e-6, load=10, max_steps=1000):\n",
        "        # Initialize the environment with given parameters\n",
        "        self.dt = dt  # Time step\n",
        "        self.vin = vin  # Input voltage\n",
        "        self.r = r  # Load resistance\n",
        "        self.l = l  # Inductor value\n",
        "        self.c = c  # Capacitor value\n",
        "        self.load = load  # Load value\n",
        "        self.max_steps = max_steps\n",
        "        \n",
        "        # Define the state and action space dimensions\n",
        "        self.state_dim = 3\n",
        "        self.action_dim = 1\n",
        "        self.action_low = np.array([0])\n",
        "        self.action_high = np.array([1])\n",
        "        self.state_low = np.array([0, 0, 0])\n",
        "        self.state_high = np.array([vin, vin, vin])\n",
        "        \n",
        "        # Initialize the state, done flag, and step count\n",
        "        self.state = np.zeros(self.state_dim)\n",
        "        self.done = False\n",
        "        self.step_count = 0\n",
        "    \n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.state = np.zeros(self.state_dim)\n",
        "        self.done = False\n",
        "        self.step_count = 0\n",
        "        return self.state\n",
        "    \n",
        "    def step(self, duty_cycle):\n",
        "        # Perform a single step in the environment given the duty cycle\n",
        "        \n",
        "        # Retrieve the previous state values\n",
        "        vout_prev, il_prev, vc_prev = self.state\n",
        "\n",
        "        # Calculate the new state values based on the duty cycle\n",
        "        vout = self.vin * duty_cycle\n",
        "        il = (self.vin - vout) / self.r\n",
        "        vc = (self.dt / self.c) * (self.load - il) + vc_prev\n",
        "\n",
        "        # Update the state with the new values\n",
        "        self.state = np.array([vout, il, vc])\n",
        "\n",
        "        # Calculate the reward based on the current state\n",
        "        reward = self.calculate_reward(vout, il, vc)\n",
        "\n",
        "        # Increment the step count\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Check if the termination condition is met\n",
        "        if self.step_count >= self.max_steps or self.condition_met():\n",
        "            self.done = True\n",
        "\n",
        "        next_state = list(np.array(self.state))\n",
        "\n",
        "        return next_state, reward, self.done, {}\n",
        "    \n",
        "    def calculate_reward(self, vout, il, vc):\n",
        "        # Calculate the reward based on the current state\n",
        "        reward = -np.abs(vout - self.vin) - 0.01 * np.abs(il) - 0.01 * np.abs(vc - self.vin)\n",
        "        return reward\n",
        "    \n",
        "    def condition_met(self):\n",
        "        # Check if the termination condition is met\n",
        "        target_voltage = 12\n",
        "        vout = self.state[0]\n",
        "        if np.abs(vout - target_voltage) > 1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "# Step 2: Deep Q-Network (DQN) Setup\n",
        "\n",
        "def create_dqn_model(state_dim, action_dim):\n",
        "    # Create a DQN model with given state and action dimensions\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(32, input_shape=(state_dim,), activation='relu'),  # Input layer\n",
        "        Dense(32, activation='relu'),  # Hidden layer\n",
        "        Dense(action_dim)  # Output layer\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create target and main DQN models\n",
        "target_network = create_dqn_model(state_dim, action_dim)\n",
        "dqn_model = create_dqn_model(state_dim, action_dim)\n",
        "\n",
        "# Define the optimizer and loss function for DQN training\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Step 3: DQN Training\n",
        "\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration rate\n",
        "epsilon_min = 0.01  # Minimum exploration rate\n",
        "batch_size = 64  # Mini-batch size\n",
        "target_update_interval = 10  # Update interval for the target network\n",
        "\n",
        "replay_buffer = []\n",
        "\n",
        "num_episodes = 1000  # Number of training episodes\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    env = BuckConverterEnvironment()\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # Exploration vs. Exploitation: Choose action based on epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.uniform(env.action_low, env.action_high)  # Explore: Choose a random action\n",
        "        else:\n",
        "            q_values = dqn_model.predict(np.expand_dims(state, axis=0))\n",
        "            action = np.squeeze(q_values)  # Exploit: Choose action based on Q-values\n",
        "        \n",
        "        # Take a step in the environment based on the chosen action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Add the experience to the replay buffer\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "        \n",
        "        if len(replay_buffer) > batch_size:\n",
        "            # Perform DQN update using a mini-batch of experiences\n",
        "            batch = random.sample(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "            \n",
        "            states = np.array(states)\n",
        "            actions = np.array(actions)\n",
        "            rewards = np.array(rewards)\n",
        "            next_states = np.array(next_states)\n",
        "            dones = np.array(dones)\n",
        "        \n",
        "            # Compute the TD targets for the Q-network update\n",
        "            q_values_next = target_network.predict(next_states)\n",
        "            targets = rewards + gamma * np.max(q_values_next, axis=1) * (1 - dones)\n",
        "        \n",
        "            with tf.GradientTape() as tape:\n",
        "                # Compute the Q-values for the current states\n",
        "                q_values = dqn_model.predict(states)\n",
        "                actions_one_hot = tf.one_hot(actions.squeeze(), depth=env.action_dim)\n",
        "                q_values_selected = tf.reduce_sum(q_values * actions_one_hot, axis=1)\n",
        "                loss = loss_fn(targets, q_values_selected)  # Compute the loss between targets and predicted Q-values\n",
        "            gradients = tape.gradient(loss, dqn_model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, dqn_model.trainable_variables))  # Apply gradients to update DQN\n",
        "        \n",
        "        if episode % target_update_interval == 0:\n",
        "            # Update target network weights periodically\n",
        "            target_network.set_weights(dqn_model.get_weights())\n",
        "        \n",
        "        state = next_state\n",
        "    \n",
        "    # Decay exploration rate after each episode\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# Step 4: Using the Trained DQN\n",
        "\n",
        "env = BuckConverterEnvironment()\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # Apply trained DQN to the environment\n",
        "    q_values = dqn_model.predict(np.expand_dims(state, axis=0))\n",
        "    action = np.squeeze(q_values)\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    state = next_state\n",
        "\n",
        "    # Render the environment\n",
        "    print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
        "\n"
      ]
    }
  ]
}