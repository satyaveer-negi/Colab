{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNtewAmrl_2K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2d2f579f-3670-4f61-f5f4-adaa5c087327"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-10e601d8721c>\u001b[0m in \u001b[0;36m<cell line: 112>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mreplay_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_episodes' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Step 1: Environment Setup\n",
        "\n",
        "class BuckConverterEnvironment:\n",
        "    def __init__(self, dt=1e-5, vin=24, r=10, l=1e-3, c=10e-6, load=10, max_steps=1000):\n",
        "        # Initialize the environment\n",
        "        self.dt = dt  # Time step\n",
        "        self.vin = vin  # Input voltage\n",
        "        self.r = r  # Load resistance\n",
        "        self.l = l  # Inductor value\n",
        "        self.c = c  # Capacitor value\n",
        "        self.load = load  # Load value\n",
        "        self.max_steps = max_steps\n",
        "        \n",
        "        self.state_dim = 3\n",
        "        self.action_dim = 1\n",
        "        self.action_low = np.array([0])\n",
        "        self.action_high = np.array([1])\n",
        "        self.state_low = np.array([0, 0, 0])\n",
        "        self.state_high = np.array([vin, vin, vin])\n",
        "        \n",
        "        self.state = np.zeros(self.state_dim)\n",
        "        self.done = False\n",
        "        self.step_count = 0\n",
        "    \n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.state = np.zeros(self.state_dim)\n",
        "        self.done = False\n",
        "        self.step_count = 0\n",
        "        return self.state\n",
        "    \n",
        "    def step(self, duty_cycle):\n",
        "    # Apply the action (duty cycle) to the converter and observe the next state and reward\n",
        "        vout_prev, il_prev, vc_prev = self.state\n",
        "\n",
        "        # Calculate new state variables\n",
        "        vout = self.vin * duty_cycle\n",
        "        il = (self.vin - vout) / self.r\n",
        "        vc = (self.dt / self.c) * (self.load - il) + vc_prev\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([vout, il, vc])\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = self.calculate_reward(vout, il, vc)\n",
        "\n",
        "        # Increment step count\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Terminate the episode if the maximum number of steps is reached\n",
        "        if self.step_count >= self.max_steps:\n",
        "            self.done = True\n",
        "\n",
        "        # Convert the next_state variable to a list of integers\n",
        "        next_state = list(np.array(self.state))\n",
        "\n",
        "        return next_state, reward, self.done, {}\n",
        "    \n",
        "    def calculate_reward(self, vout, il, vc):\n",
        "    # Placeholder reward calculation for demonstration\n",
        "        reward = -np.abs(vout - self.vin) - 0.01 * np.abs(il) - 0.01 * np.abs(vc - self.vin).__float__()\n",
        "        return reward\n",
        "    \n",
        "    def condition_met(self):\n",
        "        # Terminate the episode if the output voltage is too far from the target\n",
        "        target_voltage = 12  # Example target voltage\n",
        "        vout = self.state[0]\n",
        "        if np.abs(vout - target_voltage) > 1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "# Initialize the environment\n",
        "env = BuckConverterEnvironment()\n",
        "\n",
        "# Get the state and action dimensions\n",
        "state_dim = env.state_dim\n",
        "action_dim = env.action_dim\n",
        "\n",
        "# Step 2: Deep Q-Network (DQN) Setup\n",
        "\n",
        "def create_dqn_model(state_dim, action_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(32, input_shape=(state_dim,), activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(action_dim)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "target_network = create_dqn_model(state_dim, action_dim)\n",
        "dqn_model = create_dqn_model(state_dim, action_dim)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Step 3: DQN Training\n",
        "\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration rate\n",
        "epsilon_min = 0.01  # Minimum exploration rate\n",
        "batch_size = 64  # Mini-batch size\n",
        "target_update_interval = 10  # Update interval for the target network\n",
        "\n",
        "replay_buffer = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.uniform(env.action_low, env.action_high)\n",
        "        else:\n",
        "            q_values = dqn_model.predict(np.expand_dims(state, axis=0))\n",
        "            action = np.squeeze(q_values)\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "        \n",
        "        if len(replay_buffer) > batch_size:\n",
        "            batch = random.sample(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "            \n",
        "            states = np.array(states)\n",
        "            actions = np.array(actions)\n",
        "            rewards = np.array(rewards)\n",
        "            next_states = np.array(next_states)\n",
        "            \n",
        "            dones = np.array(dones)\n",
        "        \n",
        "            q_values_next = target_network.predict(next_states)\n",
        "            targets = rewards + gamma * np.max(q_values_next, axis=1) * (1 - dones)\n",
        "        \n",
        "            with tf.GradientTape() as tape:\n",
        "                q_values = dqn_model(states)\n",
        "                loss = loss_fn(targets, tf.reduce_sum(q_values * actions, axis=1))\n",
        "            gradients = tape.gradient(loss, dqn_model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, dqn_model.trainable_variables))\n",
        "        \n",
        "        if episode % target_update_interval == 0:\n",
        "            target_network.set_weights(dqn_model.get_weights())\n",
        "        \n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# Step 4: Using the Trained DQN\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    q_values = dqn_model.predict(np.expand_dims(state, axis=0))\n",
        "    action = np.squeeze(q_values)\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    state = next_state\n"
      ]
    }
  ]
}