{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyaveer-negi/Colab/blob/main/Buck_Boost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hNtewAmrl_2K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "0cb3018f-03a9-4242-e4f0-a216d43c65ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 210ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-cf5941ecac6b>:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cf5941ecac6b>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Compute the TD targets for the Q-network update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mq_values_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Step 1: Environment Setup\n",
        "\n",
        "class BuckConverterEnvironment:\n",
        "    def __init__(self, dt=1e-5, vin=24, r=10, l=1e-3, c=10e-6, load=10, max_steps=1000):\n",
        "        # Initialize the environment with given parameters\n",
        "        self.dt = dt  # Time step\n",
        "        self.vin = vin  # Input voltage\n",
        "        self.r = r  # Load resistance\n",
        "        self.l = l  # Inductor value\n",
        "        self.c = c  # Capacitor value\n",
        "        self.load = load  # Load value\n",
        "        self.max_steps = max_steps\n",
        "        \n",
        "        # Define the state and action space dimensions\n",
        "        self.state_dim = 3\n",
        "        self.action_dim = 1\n",
        "        self.action_low = np.array([0])\n",
        "        self.action_high = np.array([1])\n",
        "        self.state_low = np.array([0, 0, 0])\n",
        "        self.state_high = np.array([vin, vin, vin])\n",
        "        \n",
        "        # Initialize the state, done flag, and step count\n",
        "        self.state = np.zeros(self.state_dim)\n",
        "        self.done = False\n",
        "        self.step_count = 0\n",
        "    \n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.state = np.zeros(self.state_dim)\n",
        "        self.done = False\n",
        "        self.step_count = 0\n",
        "        return self.state\n",
        "    \n",
        "    def step(self, duty_cycle):\n",
        "        # Perform a single step in the environment given the duty cycle\n",
        "        \n",
        "        # Retrieve the previous state values\n",
        "        vout_prev, il_prev, vc_prev = self.state\n",
        "\n",
        "        # Calculate the new state values based on the duty cycle\n",
        "        vout = self.vin * duty_cycle\n",
        "        il = (self.vin - vout) / self.r\n",
        "        vc = (self.dt / self.c) * (self.load - il) + vc_prev\n",
        "\n",
        "        # Update the state with the new values\n",
        "        self.state = np.array([vout, il, vc])\n",
        "\n",
        "        # Calculate the reward based on the current state\n",
        "        reward = self.calculate_reward(vout, il, vc)\n",
        "\n",
        "        # Increment the step count\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Check if the termination condition is met\n",
        "        if self.step_count >= self.max_steps or self.condition_met():\n",
        "            self.done = True\n",
        "\n",
        "        next_state = list(np.array(self.state))\n",
        "\n",
        "        return next_state, reward, self.done, {}\n",
        "    \n",
        "    def calculate_reward(self, vout, il, vc):\n",
        "        # Calculate the reward based on the current state\n",
        "        reward = -np.abs(vout - self.vin) - 0.01 * np.abs(il) - 0.01 * np.abs(vc - self.vin)\n",
        "        return reward\n",
        "    \n",
        "    def condition_met(self):\n",
        "        # Check if the termination condition is met\n",
        "        target_voltage = 12\n",
        "        vout = self.state[0]\n",
        "        if np.abs(vout - target_voltage) > 1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "# Step 2: Deep Q-Network (DQN) Setup\n",
        "\n",
        "def create_dqn_model(state_dim, action_dim):\n",
        "    # Create a DQN model with given state and action dimensions\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(32, input_shape=(state_dim,), activation='relu'),  # Input layer\n",
        "        Dense(32, activation='relu'),  # Hidden layer\n",
        "        Dense(action_dim)  # Output layer\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create target and main DQN models\n",
        "target_network = create_dqn_model(state_dim, action_dim)\n",
        "dqn_model = create_dqn_model(state_dim, action_dim)\n",
        "\n",
        "# Define the optimizer and loss function for DQN training\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Step 3: DQN Training\n",
        "\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration rate\n",
        "epsilon_min = 0.01  # Minimum exploration rate\n",
        "batch_size = 64  # Mini-batch size\n",
        "target_update_interval = 10  # Update interval for the target network\n",
        "\n",
        "replay_buffer = []\n",
        "\n",
        "num_episodes = 1000  # Number of training episodes\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    env = BuckConverterEnvironment()\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # Exploration vs. Exploitation: Choose action based on epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.uniform(env.action_low, env.action_high)  # Explore: Choose a random action\n",
        "        else:\n",
        "            q_values = dqn_model.predict(np.expand_dims(state, axis=0))\n",
        "            action = np.squeeze(q_values)  # Exploit: Choose action based on Q-values\n",
        "        \n",
        "        # Take a step in the environment based on the chosen action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Add the experience to the replay buffer\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "        \n",
        "        if len(replay_buffer) > batch_size:\n",
        "            # Perform DQN update using a mini-batch of experiences\n",
        "            batch = random.sample(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        \n",
        "            # Compute the TD targets for the Q-network update\n",
        "            q_values_next = target_network.predict(next_states)\n",
        "            targets = rewards + gamma * np.max(q_values_next, axis=1) * (1 - dones)\n",
        "        \n",
        "            with tf.GradientTape() as tape:\n",
        "                # Compute the Q-values for the current states\n",
        "                q_values = dqn_model.predict(states)\n",
        "                actions_one_hot = tf.one_hot(actions.reshape(-1), depth=env.action_dim)\n",
        "                q_values_selected = tf.reduce_sum(q_values * actions_one_hot, axis=1)\n",
        "                loss = loss_fn(targets, q_values_selected)  # Compute the loss between targets and predicted Q-values\n",
        "            gradients = tape.gradient(loss, dqn_model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, dqn_model.trainable_variables))  # Apply gradients to update DQN\n",
        "        \n",
        "        if episode % target_update_interval == 0:\n",
        "            # Update target network weights periodically\n",
        "            target_network.set_weights(dqn_model.get_weights())\n",
        "        \n",
        "        state = next_state\n",
        "    \n",
        "    # Decay exploration rate after each episode\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# Step 4: Using the Trained DQN\n",
        "\n",
        "env = BuckConverterEnvironment()\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # Apply trained DQN to the environment\n",
        "    q_values = dqn_model.predict(np.expand_dims(state, axis=0))\n",
        "    action = np.squeeze(q_values)\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    state = next_state\n",
        "\n",
        "    # Print the current state and action for visualization\n",
        "    print(\"State:\", state)\n",
        "    print(\"Action:\", action)\n",
        "    print(\"Reward:\", reward)\n",
        "    print(\"Done:\", done)\n",
        "    print()\n"
      ]
    }
  ]
}